{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/noonmare/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/noonmare/anaconda3/lib/python3.9/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from dataset_preprocessing import Paths, Dataset\n",
    "import plotly.express as px\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import logging\n",
    "import random\n",
    "from metrics import confusion_matrix, accuracy_per_class\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.optim import lr_scheduler, Adam\n",
    "from sklearn.utils import shuffle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "from utils import MyDataset, FocalLoss\n",
    "from gan import Gen_ac_wgan_gp_1d, Gen_dcgan_gp_1d\n",
    "from snn import ShallowNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHANNELS_IMG = 1\n",
    "FEATURES_GEN = 120\n",
    "Z_DIM = 100\n",
    "BATCH_SIZE = 16\n",
    "IMG_SIZE = 120\n",
    "GEN_EMBEDDING = 100\n",
    "NUM_CLASSES = 20\n",
    "BATCH_SIZE_SNN = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "LOGGING_FILE = \"logs/cases.log\"\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "inc_v3_path =  Paths.pandora_18k + 'Conv_models/Inception-V3/'\n",
    "\n",
    "ds = Dataset(Paths.pandora_18k)\n",
    "\n",
    "train_path = inc_v3_path + 'train_full_emb.csv'\n",
    "valid_path = inc_v3_path + 'valid_full_emb.csv'\n",
    "test_path = inc_v3_path + 'test_full_emb.csv'\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "df_valid = pd.read_csv(valid_path)\n",
    "df_test = pd.read_csv(test_path)\n",
    "\n",
    "df = shuffle(pd.concat([df_train, df_valid], axis=0))\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, filename=LOGGING_FILE,filemode=\"a\",\n",
    "                    format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "classes = ds.classes\n",
    "\n",
    "snn_path = inc_v3_path + 'snn.pth'\n",
    "\n",
    "dataset_valid = MyDataset(df_valid, num_classes=len(ds.classes))\n",
    "\n",
    "dataset_test = MyDataset(df_test, num_classes=len(ds.classes))\n",
    "\n",
    "\n",
    "dataset_valid = MyDataset(df_valid, num_classes=len(ds.classes))\n",
    "\n",
    "dataloader_valid = torch.utils.data.DataLoader(dataset=dataset_valid, \n",
    "                                        batch_size=BATCH_SIZE_SNN, \n",
    "                                        shuffle=True, \n",
    "                                        num_workers=4,\n",
    "                                        drop_last=True)\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset=dataset_test, \n",
    "                                        batch_size=BATCH_SIZE_SNN, \n",
    "                                        shuffle=True, \n",
    "                                        num_workers=4,\n",
    "                                        drop_last=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### WGAN generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "filter_need = False\n",
    "\n",
    "fake_vectors = pd.DataFrame()\n",
    "fake_vectors_filtered = pd.DataFrame()\n",
    "\n",
    "gen_classes = range(20)\n",
    "\n",
    "for ind in gen_classes:\n",
    "\n",
    "    cl = classes[ind]\n",
    "\n",
    "    gen_path = Paths.pandora_18k + 'Generation/model/gen_' + cl + '.pkl'\n",
    "    gen_path_filtered = Paths.pandora_18k + 'Generation/model/gen_' + cl + '_filtered.pkl'\n",
    "\n",
    "    gen = Gen_dcgan_gp_1d(Z_DIM, CHANNELS_IMG, FEATURES_GEN)\n",
    "    gen_filtered = Gen_dcgan_gp_1d(Z_DIM, CHANNELS_IMG, FEATURES_GEN, filtered=True)\n",
    "\n",
    "    gen.load_state_dict(torch.load(gen_path))\n",
    "    gen_filtered.load_state_dict(torch.load(gen_path_filtered))\n",
    "    \n",
    "    gen.to(DEVICE)\n",
    "    gen_filtered.to(DEVICE)\n",
    "\n",
    "    df_cl = df.query(f\"label == {ind+1}\")\n",
    "\n",
    "\n",
    "    valid_cl = df_valid.query(f\"label == {ind+1}\")\n",
    "\n",
    "    if filter_need:\n",
    "        filtering = torch.tensor(valid_cl.mean())[range(ind*6, (ind+1)*6)]\n",
    "    else:\n",
    "        filtering = torch.zeros(6)\n",
    "\n",
    "    cl_fake_vectors = pd.DataFrame()\n",
    "    cl_fake_vectors_filtered = pd.DataFrame()\n",
    "\n",
    "    while(len(cl_fake_vectors) < len(df_cl) // 2):\n",
    "\n",
    "        noise = torch.randn((BATCH_SIZE, Z_DIM, 1)).to(DEVICE)\n",
    "\n",
    "        fake = gen(noise).squeeze()\n",
    "\n",
    "        fake_ind = []\n",
    "\n",
    "        for i in range(len(fake)):\n",
    "\n",
    "            if all(torch.abs(fake[i].detach().cpu()[range(ind*6, (ind+1)*6)] - filtering) >  0.0):\n",
    "                fake_ind.append(i)\n",
    "\n",
    "        cl_fake_vectors = pd.concat([cl_fake_vectors, pd.DataFrame(data=fake[fake_ind].detach().cpu())])\n",
    "\n",
    "    cl_fake_vectors[\"label\"] = pd.Series([ind+1 for _ in range(len(cl_fake_vectors))])\n",
    "    cl_fake_vectors.columns = df.columns\n",
    "    fake_vectors = pd.concat([fake_vectors, cl_fake_vectors])\n",
    "    \n",
    "    while(len(cl_fake_vectors_filtered) < len(df_cl) // 2):\n",
    "\n",
    "        noise = torch.randn((BATCH_SIZE, Z_DIM, 1)).to(DEVICE)\n",
    "\n",
    "        fake_filtered = gen_filtered(noise).squeeze()\n",
    "\n",
    "        fake_filtered_ind = []\n",
    "\n",
    "        for i in range(len(fake_filtered)):\n",
    "\n",
    "            if all(torch.abs(fake_filtered[i].detach().cpu()[range(ind*6, (ind+1)*6)] - filtering) > 0.0):\n",
    "                fake_filtered_ind.append(i)\n",
    "\n",
    "        cl_fake_vectors_filtered = pd.concat([cl_fake_vectors_filtered, pd.DataFrame(data=fake_filtered[fake_filtered_ind].detach().cpu())])\n",
    "\n",
    "    cl_fake_vectors_filtered[\"label\"] = pd.Series([ind+1 for _ in range(len(cl_fake_vectors_filtered))])\n",
    "    cl_fake_vectors_filtered.columns = df.columns\n",
    "    fake_vectors_filtered = pd.concat([fake_vectors_filtered, cl_fake_vectors_filtered])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional WGAN Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_fake_vectors = pd.DataFrame()\n",
    "cond_fake_vectors_filtered = pd.DataFrame()\n",
    "\n",
    "gen_cond_path = Paths.pandora_18k + 'Generation/model/gen_cond.pkl'\n",
    "gen_cond_f_path = Paths.pandora_18k + 'Generation/model/gen_cond_filtered.pkl'\n",
    "\n",
    "gen_cond = Gen_ac_wgan_gp_1d(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMG_SIZE, GEN_EMBEDDING).to(DEVICE)\n",
    "gen_cond_filtered = Gen_ac_wgan_gp_1d(Z_DIM, CHANNELS_IMG, FEATURES_GEN, NUM_CLASSES, IMG_SIZE, GEN_EMBEDDING, filtered=True).to(DEVICE)\n",
    "\n",
    "gen_cond.load_state_dict(torch.load(gen_cond_path))\n",
    "gen_cond_filtered.load_state_dict(torch.load(gen_cond_f_path))\n",
    "\n",
    "gen_cond.to(DEVICE)\n",
    "gen_cond_filtered.to(DEVICE)\n",
    "\n",
    "gen_cond.eval()\n",
    "gen_cond_filtered.eval()\n",
    "\n",
    "for ind, _ in enumerate(classes):\n",
    "\n",
    "    cl_fake_vectors = pd.DataFrame()\n",
    "    cl_fake_vectors_filtered = pd.DataFrame()\n",
    "\n",
    "    df_cl = df.query(f\"label == {ind+1}\")\n",
    "\n",
    "    for _ in range(len(df_cl) // BATCH_SIZE):\n",
    "\n",
    "        noise = torch.randn((BATCH_SIZE, Z_DIM, 1)).to(DEVICE)\n",
    "        labels = torch.tensor([ind for _ in range(BATCH_SIZE)])\n",
    "        labels = labels.type(torch.LongTensor).to(DEVICE)\n",
    "\n",
    "        fake = gen_cond(noise, labels)\n",
    "        fake_filtered = gen_cond_filtered(noise, labels)\n",
    "\n",
    "        cl_fake_vectors = pd.concat([cl_fake_vectors, pd.DataFrame(data=fake.detach().cpu().squeeze())])\n",
    "        cl_fake_vectors_filtered = pd.concat([cl_fake_vectors_filtered, pd.DataFrame(data=fake_filtered.detach().cpu().squeeze())])\n",
    "\n",
    "    cl_fake_vectors[\"label\"] = pd.Series([ind+1 for _ in range(len(cl_fake_vectors))])\n",
    "    cl_fake_vectors_filtered[\"label\"] = pd.Series([ind+1 for _ in range(len(cl_fake_vectors_filtered))])\n",
    "\n",
    "    cl_fake_vectors.columns = cl_fake_vectors_filtered.columns = df.columns\n",
    "\n",
    "    cond_fake_vectors = pd.concat([cond_fake_vectors, cl_fake_vectors.iloc[:len(df_cl) // 2]])\n",
    "    cond_fake_vectors_filtered = pd.concat([cond_fake_vectors_filtered, cl_fake_vectors_filtered.iloc[:len(df_cl) // 2]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments with not filtered vectors with WGAN with augmentation of 3 classes with the lowest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = MyDataset(shuffle(pd.concat([df, \n",
    "                                             fake_vectors.query(\"label == 6\"), \n",
    "                                             fake_vectors.query(\"label == 10\"), \n",
    "                                             fake_vectors.query(\"label == 11\"), \n",
    "                                             ], \n",
    "                                             axis=0)), num_classes=len(ds.classes))\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset=dataset_train, \n",
    "                                        batch_size=BATCH_SIZE_SNN, \n",
    "                                        shuffle=True, \n",
    "                                        num_workers=4,\n",
    "                                        drop_last=True)\n",
    "\n",
    "dataloaders = {\"train\" : dataloader_train, \"validation\" : dataloader_valid, \"test\" : dataloader_test}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 121\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "logging.info(f\"Seed {SEED}\")\n",
    "\n",
    "Net = ShallowNN().to(DEVICE)\n",
    "\n",
    "optimizer_name = \"Adam\"\n",
    "\n",
    "lr = 0.003\n",
    "\n",
    "criterion_name = \"FocalLoss\"\n",
    "\n",
    "optimizer = Adam(Net.parameters(), lr=lr, capturable=True)\n",
    "\n",
    "criterion = FocalLoss(reduction=\"mean\", gamma=2)\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\")\n",
    "\n",
    "logging.info(f\"Net parameters {Net.parameters}\")\n",
    "logging.info(f\"Optimizer :{optimizer_name}, lr : {lr}, criterion : {criterion_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 266/266 [00:02<00:00, 118.83batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 2.2166 train accuracy: 65.32%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 51/51 [00:00<00:00, 72.05batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9959 validation accuracy: 68.21%\n",
      "\n",
      "Epoch 2/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 266/266 [00:02<00:00, 88.84batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.7412 train accuracy: 89.77%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 51/51 [00:00<00:00, 87.66batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9505 validation accuracy: 69.03%\n",
      "\n",
      "Epoch 3/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 266/266 [00:02<00:00, 88.81batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.7202 train accuracy: 90.04%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 51/51 [00:00<00:00, 75.26batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9406 validation accuracy: 69.18%\n",
      "\n",
      "Epoch 4/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 266/266 [00:03<00:00, 87.95batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.7135 train accuracy: 90.14%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 51/51 [00:00<00:00, 76.12batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9318 validation accuracy: 69.57%\n",
      "\n",
      "Epoch 5/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 266/266 [00:02<00:00, 93.08batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.7093 train accuracy: 90.22%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 51/51 [00:00<00:00, 121.93batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9277 validation accuracy: 69.45%\n",
      "\n",
      "Epoch 6/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 266/266 [00:02<00:00, 89.09batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.7065 train accuracy: 90.39%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 51/51 [00:00<00:00, 116.49batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9244 validation accuracy: 70.05%\n",
      "\n",
      "Epoch 7/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 266/266 [00:02<00:00, 89.29batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.7044 train accuracy: 90.50%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 51/51 [00:00<00:00, 73.39batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9212 validation accuracy: 70.17%\n",
      "\n",
      "Epoch 8/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 266/266 [00:02<00:00, 91.70batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.7027 train accuracy: 90.54%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 51/51 [00:00<00:00, 71.02batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9204 validation accuracy: 70.08%\n",
      "\n",
      "Epoch 9/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 266/266 [00:02<00:00, 95.89batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.7014 train accuracy: 90.58%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 51/51 [00:00<00:00, 67.92batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9190 validation accuracy: 70.23%\n",
      "\n",
      "Epoch 10/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 266/266 [00:02<00:00, 91.84batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.7002 train accuracy: 90.65%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 51/51 [00:00<00:00, 122.95batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9170 validation accuracy: 70.17%\n",
      "\n",
      "Epoch 11/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 266/266 [00:02<00:00, 90.12batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.6992 train accuracy: 90.70%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 51/51 [00:00<00:00, 82.94batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9130 validation accuracy: 70.35%\n",
      "\n",
      "Epoch 12/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 266/266 [00:02<00:00, 93.21batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.6985 train accuracy: 90.66%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 51/51 [00:00<00:00, 85.85batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9107 validation accuracy: 70.59%\n",
      "\n",
      "Epoch 13/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 266/266 [00:02<00:00, 100.45batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.6972 train accuracy: 90.78%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 51/51 [00:00<00:00, 66.89batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9111 validation accuracy: 70.65%\n",
      "\n",
      "Epoch 14/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 266/266 [00:02<00:00, 98.79batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.6972 train accuracy: 90.81%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 51/51 [00:00<00:00, 103.91batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9111 validation accuracy: 70.56%\n",
      "\n",
      "Epoch 15/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 266/266 [00:02<00:00, 91.06batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.6971 train accuracy: 90.82%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 51/51 [00:00<00:00, 72.74batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9097 validation accuracy: 70.80%\n",
      "\n",
      "Epoch 16/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 266/266 [00:02<00:00, 89.29batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.6969 train accuracy: 90.83%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 51/51 [00:00<00:00, 101.63batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9109 validation accuracy: 70.68%\n",
      "\n",
      "Epoch 17/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 266/266 [00:02<00:00, 89.20batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.6969 train accuracy: 90.82%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 51/51 [00:00<00:00, 65.26batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9106 validation accuracy: 70.62%\n",
      "\n",
      "Epoch 18/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 266/266 [00:02<00:00, 95.24batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.6968 train accuracy: 90.83%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 51/51 [00:00<00:00, 72.12batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9102 validation accuracy: 70.62%\n",
      "\n",
      "Epoch 19/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 266/266 [00:02<00:00, 101.03batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.6967 train accuracy: 90.81%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 51/51 [00:00<00:00, 67.78batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9093 validation accuracy: 70.71%\n",
      "\n",
      "Epoch 20/20\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 266/266 [00:02<00:00, 89.58batch/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss : 1.6964 train accuracy: 90.82%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 51/51 [00:00<00:00, 65.03batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation loss : 1.9094 validation accuracy: 70.68%\n",
      "\n",
      "Training complete in 1m 10s\n",
      "Best validation accuracy: 70.80%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "statistics_data = {\n",
    "            'number of epochs' : range(1,EPOCHS+1),\n",
    "            'training loss' : [],\n",
    "            'validation loss' : [],\n",
    "            'training accuracy' : [],\n",
    "            'validation accuracy' : []\n",
    "        }\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_acc = 0.0\n",
    "best_model_wts = copy.deepcopy(Net.state_dict())\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'validation']:\n",
    "        if phase == 'train':\n",
    "            Net.train()  # Set model to training mode\n",
    "        else:\n",
    "            Net.eval()   # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        with tqdm(dataloaders[phase], unit='batch') as tepoch:\n",
    "            for inputs, labels in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.type(torch.LongTensor).to(DEVICE)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = torch.squeeze(Net(inputs))\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step(0.005)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            if phase == 'train':\n",
    "                statistics_data['training loss'].append(epoch_loss)\n",
    "                statistics_data['training accuracy'].append(epoch_acc.cpu().numpy())\n",
    "            else:\n",
    "                statistics_data['validation loss'].append(epoch_loss)\n",
    "                statistics_data['validation accuracy'].append(epoch_acc.cpu().numpy())\n",
    "\n",
    "            if phase == 'validation' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(Net.state_dict())\n",
    "            \n",
    "            print(f'{phase} loss : {epoch_loss:.4f} {phase} accuracy: {epoch_acc*100:.2f}%')\n",
    "            logging.info(f'{phase} loss : {epoch_loss:.4f} {phase} accuracy: {epoch_acc*100:.2f}%')\n",
    "\n",
    "        print()\n",
    "\n",
    "time_elapsed = time.time() - start_time\n",
    "print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "logging.info(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "print(f'Best validation accuracy: {best_acc*100:.2f}%')\n",
    "logging.info(f'Best validation accuracy: {best_acc*100:.2f}%')\n",
    "\n",
    "Net.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 70.335 %\n"
     ]
    }
   ],
   "source": [
    "target = torch.tensor([], dtype=torch.int32).to(DEVICE)\n",
    "pred = torch.tensor([], dtype=torch.int32).to(DEVICE)\n",
    "\n",
    "for images, labels in dataloaders[\"test\"]:\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "\n",
    "    target = torch.cat((target, labels))\n",
    "\n",
    "    outputs = Net(images)\n",
    "    _, predictions = torch.max(outputs, 2)\n",
    "    predictions = torch.squeeze(predictions, 1)\n",
    "\n",
    "    pred = torch.cat((pred, predictions))\n",
    "\n",
    "target, pred = target.to(torch.int32).cpu(), pred.to(torch.int32).cpu()\n",
    "\n",
    "print(f\"Accuracy : {round(accuracy_score(target, pred) * 100, 3)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for 01_Byzantin_Iconography: 94.7 %\n",
      "Accuracy for 02_Early_Renaissance: 79.0 %\n",
      "Accuracy for 03_Northern_Renaissance: 85.1 %\n",
      "Accuracy for 04_High_Renaissance: 73.9 %\n",
      "Accuracy for 05_Baroque: 59.0 %\n",
      "Accuracy for 06_Rococo: 47.6 %\n",
      "Accuracy for 07_Romanticism: 54.2 %\n",
      "Accuracy for 08_Realism: 71.4 %\n",
      "Accuracy for 09_Impressionism: 74.1 %\n",
      "Accuracy for 10_Post_Impressionism: 61.7 %\n",
      "Accuracy for 11_Expressionism: 47.0 %\n",
      "Accuracy for 12_Symbolism: 63.4 %\n",
      "Accuracy for 13_Fauvism: 55.8 %\n",
      "Accuracy for 14_Cubism: 76.4 %\n",
      "Accuracy for 15_Surrealism: 64.1 %\n",
      "Accuracy for 16_AbstractArt: 70.1 %\n",
      "Accuracy for 17_NaiveArt: 67.1 %\n",
      "Accuracy for 18_PopArt: 73.2 %\n",
      "Accuracy for 19_ChineseArt: 77.0 %\n",
      "Accuracy for 20_JapaneseArt: 98.6 %\n"
     ]
    }
   ],
   "source": [
    "acc_per_class = accuracy_per_class(pred, target, ds.classes)\n",
    "\n",
    "for style, acc in acc_per_class.items():\n",
    "    print(f'Accuracy for {style}: {acc:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean accuracy for min 3 styles : 49.6 %\n",
      "Accuracy for 11_Expressionism: 47.0 %\n",
      "Accuracy for 06_Rococo: 47.6 %\n",
      "Accuracy for 07_Romanticism: 54.2 %\n"
     ]
    }
   ],
   "source": [
    "x = sorted(list(acc_per_class.items()), key=lambda x : x[1])[:3]\n",
    "\n",
    "print(f\"Mean accuracy for min 3 styles : {sum([el[1] for el in x]) / 3:.1f} %\")\n",
    "\n",
    "for style, acc in x:\n",
    "    print(f'Accuracy for {style}: {acc:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
