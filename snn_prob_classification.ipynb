{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision import models\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import time\n",
    "import copy\n",
    "import logging\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from torchvision import datasets\n",
    "from torch.optim import lr_scheduler\n",
    "from sklearn.utils import shuffle\n",
    "import torch.nn.functional as F\n",
    "from metrics import confusion_matrix, accuracy_per_class\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from dataset_preprocessing import Paths, Dataset\n",
    "from snn import ShallowNN\n",
    "from utils import FocalLoss, MyDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "inc_v3_path =  Paths.pandora_18k + 'Conv_models/Inception-V3/'\n",
    "\n",
    "train_path = inc_v3_path + 'train_full_emb.csv'\n",
    "valid_path = inc_v3_path + 'valid_full_emb.csv'\n",
    "test_path = inc_v3_path + 'test_full_emb.csv'\n",
    "mistakes_path = inc_v3_path + 'mistakes.csv'\n",
    "\n",
    "fake_Baroque_path = inc_v3_path + 'fake_emb_Baroque.csv'\n",
    "fake_Rococo_path = inc_v3_path + 'fake_emb_Rococo.csv'\n",
    "fake_Romanticism_path = inc_v3_path + 'fake_emb_Romanticism.csv'\n",
    "fake_Expressionism_path = inc_v3_path + 'fake_emb_Expressionism.csv'\n",
    "fake_PostImpressionism_path = inc_v3_path + 'fake_emb_Post-Impressionism.csv'\n",
    "\n",
    "train_prob =  inc_v3_path + 'train_prob.csv'\n",
    "valid_prob =  inc_v3_path + 'valid_prob.csv'\n",
    "test_prob =  inc_v3_path + 'test_prob.csv'\n",
    "\n",
    "model_save_path = inc_v3_path + 'snn.pth'\n",
    "\n",
    "logging_file = \"logs/snn.log\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = Dataset(Paths.pandora_18k)\n",
    "\n",
    "df_train = pd.read_csv(train_path)\n",
    "\n",
    "df_valid = pd.read_csv(valid_path)\n",
    "\n",
    "df_test = pd.read_csv(test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "\n",
    "dataset_train = MyDataset(shuffle(pd.concat([df_train, df_valid], axis=0)), num_classes=len(ds.classes))\n",
    "\n",
    "dataset_valid = MyDataset(df_valid, num_classes=len(ds.classes))\n",
    "\n",
    "dataset_test = MyDataset(df_test, num_classes=len(ds.classes))\n",
    "\n",
    "dataloader_train = torch.utils.data.DataLoader(dataset=dataset_train, \n",
    "                                        batch_size=BATCH_SIZE, \n",
    "                                        shuffle=True, \n",
    "                                        num_workers=4,\n",
    "                                        drop_last=True)\n",
    "\n",
    "dataset_valid = MyDataset(df_valid, num_classes=len(ds.classes))\n",
    "\n",
    "dataloader_valid = torch.utils.data.DataLoader(dataset=dataset_valid, \n",
    "                                        batch_size=BATCH_SIZE, \n",
    "                                        shuffle=True, \n",
    "                                        num_workers=4,\n",
    "                                        drop_last=True)\n",
    "\n",
    "dataloader_test = torch.utils.data.DataLoader(dataset=dataset_test, \n",
    "                                        batch_size=BATCH_SIZE, \n",
    "                                        shuffle=True, \n",
    "                                        num_workers=4,\n",
    "                                        drop_last=True)\n",
    "\n",
    "dataloaders = {\"train\" : dataloader_train, \"validation\" : dataloader_valid, \"test\" : dataloader_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 121\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, filename=logging_file,filemode=\"a\",\n",
    "                    format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "logging.info(f\"Seed {SEED}\")\n",
    "\n",
    "Net = ShallowNN().to(DEVICE)\n",
    "\n",
    "optimizer_name = \"Adam\"\n",
    "\n",
    "lr = 0.003\n",
    "\n",
    "criterion_name = \"FocalLoss\"\n",
    "\n",
    "#criterion_name = \"CrossEntropy\"\n",
    "\n",
    "optimizer = Adam(Net.parameters(), lr=lr, capturable=True)\n",
    "\n",
    "criterion = FocalLoss(reduction=\"mean\", gamma=2)\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\")\n",
    "\n",
    "logging.info(f\"Net parameters {Net.parameters}\")\n",
    "logging.info(f\"Optimizer :{optimizer_name}, lr : {lr}, criterion : {criterion_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "summary(Net, input_size=(BATCH_SIZE, 1, 120))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "\n",
    "statistics_data = {\n",
    "            'number of epochs' : range(1,EPOCHS+1),\n",
    "            'training loss' : [],\n",
    "            'validation loss' : [],\n",
    "            'training accuracy' : [],\n",
    "            'validation accuracy' : []\n",
    "        }\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "best_acc = 0.0\n",
    "best_model_wts = copy.deepcopy(Net.state_dict())\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f'Epoch {epoch}/{EPOCHS}')\n",
    "    print('-' * 10)\n",
    "\n",
    "    # Each epoch has a training and validation phase\n",
    "    for phase in ['train', 'validation']:\n",
    "        if phase == 'train':\n",
    "            Net.train()  # Set model to training mode\n",
    "        else:\n",
    "            Net.eval()   # Set model to evaluate mode\n",
    "\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "\n",
    "        # Iterate over data.\n",
    "        with tqdm(dataloaders[phase], unit='batch') as tepoch:\n",
    "            for inputs, labels in tepoch:\n",
    "                tepoch.set_description(f\"Epoch {epoch}\")\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.type(torch.LongTensor).to(DEVICE)\n",
    "                \n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = torch.squeeze(Net(inputs))\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                scheduler.step(0.005)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "\n",
    "            if phase == 'train':\n",
    "                statistics_data['training loss'].append(epoch_loss)\n",
    "                statistics_data['training accuracy'].append(epoch_acc.cpu().numpy())\n",
    "            else:\n",
    "                statistics_data['validation loss'].append(epoch_loss)\n",
    "                statistics_data['validation accuracy'].append(epoch_acc.cpu().numpy())\n",
    "\n",
    "            if phase == 'validation' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(Net.state_dict())\n",
    "            \n",
    "            print(f'{phase} loss : {epoch_loss:.4f} {phase} accuracy: {epoch_acc*100:.2f}%')\n",
    "            logging.info(f'{phase} loss : {epoch_loss:.4f} {phase} accuracy: {epoch_acc*100:.2f}%')\n",
    "\n",
    "        print()\n",
    "\n",
    "time_elapsed = time.time() - start_time\n",
    "print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "logging.info(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "print(f'Best validation accuracy: {best_acc*100:.2f}%')\n",
    "logging.info(f'Best validation accuracy: {best_acc*100:.2f}%')\n",
    "\n",
    "Net.load_state_dict(best_model_wts)\n",
    "torch.save(Net.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mistakes_train = pd.DataFrame()\n",
    "Mistakes_valid = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(121)\n",
    "\n",
    "Net.eval()\n",
    "\n",
    "target = torch.tensor([], dtype=torch.float32).to(DEVICE)\n",
    "pred = torch.tensor([], dtype=torch.float32).to(DEVICE)\n",
    "\n",
    "for images, labels in dataloaders[\"test\"]:\n",
    "    images = images.to(DEVICE)\n",
    "    labels = labels.to(DEVICE)\n",
    "\n",
    "    target = torch.cat((target, labels))\n",
    "\n",
    "    outputs = Net(images)\n",
    "    _, predictions = torch.max(outputs, 2)\n",
    "    predictions = torch.squeeze(predictions, 1)\n",
    "\n",
    "    pred = torch.cat((pred, predictions))\n",
    "\n",
    "target, pred = target.to(torch.int32).cpu(), pred.to(torch.int32).cpu()\n",
    "\n",
    "print(f\"Accuracy : {round(accuracy_score(target, pred) * 100, 3)} %\")\n",
    "logging.info(f\"Accuracy : {round(accuracy_score(target, pred) * 100, 3)} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_per_class = accuracy_per_class(pred, target, ds.classes)\n",
    "\n",
    "for style, acc in acc_per_class.items():\n",
    "    print(f'Accuracy for {style}: {acc:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(pred, target, ds.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sorted(list(acc_per_class.items()), key=lambda x : x[1])[:3]\n",
    "\n",
    "print(f\"Mean accuracy for min 3 styles : {sum([el[1] for el in x]) / 3:.1f} %\")\n",
    "\n",
    "for style, acc in x:\n",
    "    print(f'Accuracy for {style}: {acc:.1f} %')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
